# Zero-shot-s2: Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models

## Overview

As image generators produce increasingly realistic images, concerns about their potential misuse continue to grow. Traditional supervised detection methods often rely on large, curated datasets and can struggle to generalize across diverse image generators. This repository contains the code and experimental results for the paper "Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models."

In this work, we investigate the use of pre-trained Vision-Language Models (VLMs) for the zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit some task-specific reasoning capabilities, and chain-of-thought (CoT) prompting offers performance gains, we demonstrate that task-aligned prompting elicits more focused reasoning and significantly improves detection performance without requiring any model fine-tuning.

Specifically, we introduce zero-shot-s² (zero-shot style and synthesis), a method that involves appending the phrase "let's examine the style and the synthesis artifacts" to the prompt. This approach boosts Macro F1 scores by 8%-29% for two widely used open-source VLMs (Qwen2.5-7B and Llama-3.2-11B). These improvements are consistent across three recent and diverse datasets (D3, DF40, GenImage), which span human faces, objects, and animals, with images generated by 16 different models, demonstrating strong generalization. We further show that zero-shot-s² offers benefits across various model sizes and scales more effectively with self-consistency than standard CoT prompting in most cases.

Our findings highlight that task-aligned prompts can effectively elicit and enhance latent capabilities in VLMs for tasks like AI-generated image detection, offering a simple, generalizable, and explainable alternative to supervised methods.

## Table of Contents

- [Repository Structure](#repository-structure)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Create a Virtual Environment](#2-create-a-virtual-environment)
  - [3. Install Dependencies](#3-install-dependencies)
  - [4. Download NLTK Resources](#4-download-nltk-resources)
  - [5. Data Preparation](#5-data-preparation)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Running Experiments](#running-experiments)
    - [Example: Evaluating Qwen2.5 7B on GenImage (2k sample)](#example-evaluating-qwen25-7b-on-genimage-2k-sample)
  - [Generating Result Tables and Plots](#generating-result-tables-and-plots)
    - [Example: Generating the Scaling Consistency Plot](#example-generating-the-scaling-consistency-plot)
  - [Downloading and Preprocessing D3 Dataset Images](https://www.google.com/search?q=%23downloading-and-preprocessing-d3-dataset-images)
- [Expected Outputs](#expected-outputs)
- [Troubleshooting](#troubleshooting)
- [License](#license)
- [Citation](#citation)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)
- [Changelog](#changelog)

## Repository Structure

```

Zero-shot-s2/
├── config.py            \# Central configuration for paths and global parameters
├── data/                \# Placeholder for input datasets (managed via .gitignore)
│   ├── d3/              \# D3 dataset images and metadata CSV
│   ├── df40/            \# DF40 dataset CSVs
│   └── genimage/        \# GenImage dataset CSVs
├── experiments/         \# Scripts for running evaluations and data processing
│   ├── evaluate\_AI\_llama.py
│   ├── evaluate\_AI\_qwen.py
│   └── load\_d3.py
├── outputs/             \# Default location for generated results, plots, tables
│   ├── responses/       \# Raw model responses and rationales (JSONL)
│   ├── scores/          \# Evaluation scores (JSON, CSV)
│   ├── plots/           \# Generated plots (PNG, PDF)
│   └── tables/          \# Generated LaTeX tables (.tex)
├── results/             \# Scripts for generating tables and plots from experiment outputs
│   ├── combine\_tables.py
│   ├── distinct\_words.py
│   ├── find\_images.py
│   ├── macro\_f1\_bars.py
│   ├── model\_size\_table.py
│   ├── prompt\_table.py
│   ├── recall\_subsets\_table.py
│   └── scaling\_consistency.py
├── utils/               \# Utility scripts and shared helper functions
│   └── helpers.py       \# Core helper functions for data loading, evaluation, etc.
├── .gitignore           \# Specifies intentionally untracked files (e.g., large data files)
├── LICENSE.md           \# Project license (To be added)
├── README.md            \# This file
└── requirements.txt     \# Python dependencies

````

## Prerequisites

* Python (e.g., 3.8+ recommended, verify based on `requirements.txt`)
* `pip` (Python package installer)
* (Optional) Conda for environment management
* (Optional but Recommended for speed) NVIDIA GPU with CUDA installed for model inference (refer to PyTorch and Transformers documentation for compatible CUDA versions).

## Setup

### 1. Clone the Repository

```bash
git clone [https://github.com/Zoher15/Zero-shot-s2.git](https://github.com/Zoher15/Zero-shot-s2.git)
cd Zero-shot-s2
````

### 2\. Create a Virtual Environment

It is highly recommended to use a virtual environment to manage dependencies.

**Using `venv`:**

```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

**Using `conda`:**

```bash
conda create -n zeroshot_s2 python=3.9  # Or your preferred Python version
conda activate zeroshot_s2
```

### 3\. Install Dependencies

Install the required Python packages using the provided `requirements.txt` file:

```bash
pip install -r requirements.txt
```

### 4\. Download NLTK Resources

Some scripts (e.g., `results/distinct_words.py`) require NLTK resources like WordNet and stopwords. The `utils/helpers.py` script attempts to download them if not found, but you can also pre-download them:

```bash
python -m nltk.downloader wordnet stopwords punkt
```

Alternatively, running a script like `results/distinct_words.py` once will trigger the automatic download if resources are missing.

### 5\. Data Preparation

This project requires several datasets. Due to their size, they are not included directly in the repository. You need to download them and organize them according to the structure defined in `config.py`. By default, scripts expect datasets to be in the `Zero-shot-s2/data/` directory.

The expected structure within `data/` is:

```
Zero-shot-s2/
└── data/
    ├── d3/
    │   └── 2k_sample_ids_d3.csv # CSV for D3 image IDs (actual images downloaded by load_d3.py)
    │   └── (images will be saved here by load_d3.py)
    ├── df40/
    │   ├── 10k_sample_df40.csv
    │   └── 2k_sample_df40.csv
    └── genimage/
        ├── 10k_random_sample.csv
        └── 2k_random_sample.csv
```

  * **D3 Dataset**:
      * The D3 dataset requires an ID CSV file. By default, `config.py` is set up to use `data/d3/2k_sample_ids_d3.csv` (defined in `config.D3_CSV_FILE`). Ensure this file exists at this location or update `config.py` accordingly. The script will save downloaded images into the directory specified by `config.D3_DIR` (default: `data/d3/`). The CSV file must contain a column named 'id' (or you can specify a different column name using the `--id_column_name` argument when running the script).
      * Use the `experiments/load_d3.py` script (see [Downloading and Preprocessing D3 Dataset Images](https://www.google.com/search?q=%23downloading-and-preprocessing-d3-dataset-images)) to download the actual images.
  * **Other Datasets (DF40, GenImage)**:
      * Place the respective CSV files in their designated directories as shown above (`data/df40/` and `data/genimage/`). The specific filenames for CSVs are defined in `config.py` (e.g., `config.GENIMAGE_2K_CSV_FILE`, `config.DF40_2K_CSV_FILE`). The image paths within these CSVs should be relative to a common base or be absolute paths that your system can access.
  * **Large Data Files**: Large data files and directories within `data/` (like image folders) should be added to your local `.gitignore` file if they are not already, to prevent accidental versioning. The provided `.gitignore` may already cover common patterns.

*(Consider adding specific download links or detailed instructions for acquiring each dataset if publicly available and permissible.)*

## Configuration

This project uses a central `config.py` file located at the root of the repository to manage all important paths and global parameters.

  * **Paths**: All input data paths (e.g., `config.DATA_DIR`, `config.D3_CSV_FILE`), output paths (`config.OUTPUTS_DIR`, `config.RESPONSES_DIR`, `config.PLOTS_DIR`), and cache paths (`config.CACHE_DIR`) are defined here.
  * **Parameters**: Some global evaluation parameters, like `config.EVAL_QUESTION_PHRASE`, are also set in `config.py`.

If you organize your data differently or want to change output locations, you should primarily modify the relevant path variables in `config.py`. There should be no need to change hardcoded paths within individual scripts.

## Usage

All scripts should be run from the root directory of the repository (`Zero-shot-s2/`).

### Running Experiments

The `experiments/` directory contains scripts to run evaluations on different models and datasets. These scripts use `utils/helpers.py` for argument parsing, data loading, and saving results, and `config.py` for path management.

**Example: Evaluating Qwen2.5 7B on the GenImage dataset (2k sample)**

```bash
python experiments/evaluate_AI_qwen.py \
    -llm qwen25-7b \
    -c 0 \
    -d genimage2k \
    -b 20 \
    -n 1 \
    -m zeroshot-2-artifacts
```

**Common Arguments for Evaluation Scripts:**

  * `-llm` or `--llm`: Model identifier (e.g., `qwen25-7b`, `llama3-11b`). Refer to the `model_dict` within the respective evaluation script for available models.
  * `-c` or `--cuda`: CUDA device ID(s) (e.g., `0`, or `0,1` for multiple GPUs, though multi-GPU support depends on the script's implementation).
  * `-d` or `--dataset`: Dataset identifier (e.g., `genimage`, `genimage2k`, `d3`, `d32k`, `df40`, `df402k`). These keys map to data loading routines and paths defined in `config.py` and `utils/helpers.py`.
  * `-b` or `--batch_size`: Batch size for model inference.
  * `-n` or `--num`: Number of sequences to generate (e.g., for self-consistency).
  * `-m` or `--mode`: Mode of reasoning or prompting strategy (e.g., `zeroshot`, `zeroshot-cot`, `zeroshot-2-artifacts`).

Refer to the `helpers.get_evaluation_args_parser()` function in `utils/helpers.py` and the `argparse` setup within each evaluation script for a complete list of options and their default values.

### Generating Result Tables and Plots

The `results/` directory contains scripts to process the outputs of experiments (stored in `outputs/responses/` and `outputs/scores/`) and generate tables (to `outputs/tables/`) and plots (to `outputs/plots/`).

Ensure that the experiment outputs are available in the locations specified by `config.py` before running these scripts.

**Example: Generating the Scaling Consistency Plot**
(Assumes `TARGET_LLAMA_MODEL_NAME = "llama3-11b"` is set correctly in `results/scaling_consistency.py` or refactored to be configurable, and relevant rationale files from evaluations with varying `n` values are present in `outputs/responses/`)

```bash
python results/scaling_consistency.py
```

This will generate a plot like `self_consistency_scaling.png` in the `outputs/plots/` directory (path configured by `config.PLOTS_DIR`).

*(For other result scripts, you may need to check their internal configurations or provide command-line arguments if they support them. Future work could standardize argument parsing for result scripts.)*

### Downloading and Preprocessing D3 Dataset Images

The `experiments/load_d3.py` script is used to download and save images for the D3 dataset based on a CSV file containing image IDs.

```bash
python experiments/load_d3.py
```

The script uses default paths for the CSV file (`config.D3_CSV_FILE`) and save directory (`config.D3_DIR`) as defined in `config.py`. If your files are located elsewhere, or if you are using a different CSV file, you can specify them using command-line arguments:

```bash
python experiments/load_d3.py \
    --ids_csv_filepath path/to/your/d3_ids.csv \
    --id_column_name your_id_column \
    --save_directory path/to/your/d3_images_save_location \
    --timeout 15 \
    --force    # Optional: to overwrite existing images
    --verbose  # Optional: for debug logging
```

A log file (`load_d3_processing.log` by default, path configured by `config.LOAD_D3_LOG_FILE`) will be created in the project root.

## Expected Outputs

  * **Experiment Scripts (`experiments/evaluate_AI_*.py`)**:
      * Generate detailed rationale files (JSONL format) in the directory specified by `config.RESPONSES_DIR`.
      * Generate score files (JSON for confusion matrix, CSV for Macro F1) in the directory specified by `config.SCORES_DIR`.
  * **Result Scripts (`results/*.py`)**:
      * Generate `.png` plot files in `config.PLOTS_DIR`.
      * Generate `.tex` LaTeX table files in `config.TABLES_DIR`.
      * Some scripts might print tables or summaries to the console.
  * **Data Loading Scripts (`experiments/load_d3.py`)**:
      * Download images to the specified save directory (e.g., `data/d3/`).
      * Produce a log file detailing the download process.

## Troubleshooting

  * **`FileNotFoundError`**:
      * Ensure that all required dataset files (CSVs, image directories) are correctly placed as per the "Data Preparation" section and that paths in `config.py` point to them correctly. Pay attention to directory and file casing, especially on case-sensitive systems like Linux.
      * Verify that output directories specified in `config.py` are writable.
  * **`ModuleNotFoundError`**:
      * Make sure your virtual environment is activated.
      * Verify that all dependencies from `requirements.txt` have been installed correctly (`pip install -r requirements.txt`).
      * Ensure you are running scripts from the project's root directory (`Zero-shot-s2/`).
  * **CUDA Errors / GPU Issues**:
      * Ensure PyTorch is installed with the correct CUDA version matching your system's CUDA drivers and GPU architecture.
      * Check if the correct GPU is visible and selected (e.g., via the `-c` argument in evaluation scripts or `CUDA_VISIBLE_DEVICES` environment variable).
  * **NLTK `LookupError`**:
      * Run the NLTK resource download command provided in the "Setup" section. Some scripts might also attempt to download these resources automatically if missing.
  * **Incorrect File Paths in Scripts**:
      * This should be minimal if `config.py` is used correctly. If you suspect a path issue within a script, verify it's using a variable from `config.py` rather than a hardcoded path.

## License

*(To be added. Choose a license, e.g., MIT, Apache 2.0, and add a `LICENSE.md` file. Example below assumes MIT License.)*

This project is licensed under the MIT License - see the `LICENSE.md` file for details.

## Citation

If you use this code or these findings in your research, please cite:

```bibtex
@misc{yourproject_neurips2025_submission,
  author    = {Anonymous Author(s)},
  title     = {Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models},
  year      = {2024},
  howpublished = {Submitted to NeurIPS 2025.}
}
```

*(Please replace with your actual BibTeX entry when available.)*

## Contributing

We welcome contributions\! If you'd like to contribute, please follow these guidelines:

  * For major changes, please open an issue first to discuss what you would like to change.
  * Ensure your code adheres to the project's coding style (consider using a linter/formatter like Black).
  * Make sure to update documentation and tests as appropriate.
  * Create a pull request with a clear description of your changes.

## Acknowledgements

*(Optional: Acknowledge any libraries, datasets, or individuals that significantly helped your project.)*